
Все сортировки можно классифицировать по их свойствам:
## Устойчивость
*Stable-sort* или *устойчивая сортировка* - сортировка, которая гарантированно не поменяет местами одинаковые ключи. Если два ключа обладали одинаковыми значениями, то они останутся в том же порядке.
### Примеры устойчивых сортировок:
* Сортировка слиянием (Merge-sort)
* Сортировка пузырьком
* Сортировка вставкой

## Локальность
*Inplace-sort* или *локальная сортировка* -  сортировка, которая использует меньше, чем $O(log^kn)$ памяти, включая рекурсивные вызовы, где $n$ - количество элементов в сортируемом массиве, а $k$ - какая-то константа.
### Примеры локальных сортировок:
* Быстрая сортировка (Quick-sort или Qsort)
* Сортировка пузырьком
* Сортировка выбором
* Сортировка вставкой
## Почему сортировка сравнением не может работать быстрее, чем  за $\Omega(n\log n)$

### Доказательство для детерминированного алгоритма
*Можем ограничиться доказательством для массива со всеми различными элементами, так как используем оценку*$\Omega$.

Распишем определение $\Omega(n\log n)$:
$$\exists c > 0:\forall  n> N\; \exists input:\; |input|=n\;\; t(input) \geqslant c\cdot n\log{n}$$
* $c$ - некоторая константа
* $n$ - размер входных данных
* $input$ - входные данные
* $t(input)$ - время работы алгоритма на входных данных $input$

Представим ход работы алгоритма, как бинарное дерево, которое разветвляется, когда мы сравниваем два элемента (алгоритм полностью детерминирован, кроме моментов сравнения):
	* Пусть вершина дерева соответствует операции $a_i < a_j$, где $a_i$, $a_j$ - элементы массива, который мы хотим отсортировать
	* Если условие выполнено ($a_i < a_j$), то переходим в левого ребенка этой вершины.
	* Если условие не выполнено ($a_i \geqslant a_j$), то переходим в правого ребенка этой вершины.
![Дерево сравнений]({% static 'articles/alds/sort_algorithms/diagram_0.svg' %})

Если для разных перестановок элементов массива алгоритм совершит одни и те же действия, то результат будет различаться, чего не может быть $\Rightarrow$ для любых двух перестановок алгоритмы будут различны $\Rightarrow$ в таком дереве хотя бы $n!$ листьев ($n!$ - количество различных перестановок).
В каждом листе бинарного дерева находится какая-то из $n!$ перестановок изначального массива, которая показывает, в каком порядке стоят элементы после сортировки, относительно того как они стояли изначально $\Rightarrow$ существует лист, глубина которого $\geqslant\log n! \geqslant c \cdot n \log n$, так как $n! \geqslant \frac{n}{2}^{\frac{n}{2}}$.

### Доказательство для недетерминированного алгоритма
Распишем определение $\Omega(n\log n)$:
$$\exists c > 0:\forall  n> N\; \exists input:\; |input|=n\;\; E(t(input)) \geqslant c\cdot n\log{n}$$
* $c$ - некоторая константа
* $n$ - размер входных данных
* $input$ - входные данные
* $E(t(input))$ - математическое ожидание времени работы алгоритма на входных данных $input$, где математическое ожидание - это среднее по всем наборам случайных значений $random$.

Введем обозначения:
* $\max\limits_{x} (f(x))$ - максимальное значение функции $f(x)$ по всем $x$
* $avg_x(f(x))$ - среднее значение функции $f(x)$ по всем $x$

И запишем определение в другом виде:
$$\max\limits_{input} (avg_{random} t(input, random)) \geqslant c\cdot n\log{n}$$
Так как $\max\limits_{x} (f(x)) \geqslant avg_x(f(x))$, то верно:
$$\max\limits_{input} (avg_{random} t(input, random)) \geqslant avg_{input} (avg_{random} t(input, random))$$
Можем поменять местами $avg_{random}$ и $avg_{input}$, так как от этого формула не изменится:
$$\max\limits_{input} (avg_{random} t(input, random)) \geqslant avg_{random} (avg_{input} t(input, random))$$
Теперь random зафиксирован, то есть определена последовательность случайных значений $\Rightarrow$ алгоритм становится детерминированным.
Вспомним бинарное дерево, которое мы строили в доказательстве для детерменированных алгоритмов, теперь нужно доказать, что средняя глубина по всем $n!$ листьям ($p$ - лист или перестановка):
$$avg_p d \geqslant c\cdot n\log{n}$$
Рассмотрим глубину $d = \log n! - 1$. На такой глубине листьев может быть не больше, чем $2^{\log n! - 1} \leq \frac{n!}{2} \Rightarrow$ на глубине меньше, чем $\log n! - 1$ может находится максимум половина листье $\Rightarrow$ вторая половина листьев находится хотя бы на глубине $\log n!$ $\Rightarrow$ средняя глубина по всем листьям хотя бы $\log \frac{n}{2}$ $\Rightarrow$ если в дереве $n!$ листьев, то средняя глубина хотя бы $c\cdot n\log{n}$:
$$avg_{input} t(input, random) \geqslant c\cdot n\log{n}$$
Тогда определение $\Omega(n\log n)$ доказано, так как верно:
$$avg_{random} (avg_{input} t(input, random)) \geqslant c\cdot n\log{n}$$

## Сортировки, которые обращаются к значению своего элемента
### Сортировка подсчетом (Count sort)
Сортировка, которая для каждого элемента вычисляет, сколько раз он встречается
* $a_1, a_2, ..., a_n$ - последовательность целых чисел, которую нужно отсортировать
* $[0, U-1]$ - диапазон, в котором лежат все $a_i$, причем $U = 2^k$

Так как $U = 2^k$, то можно считать, что мы сортируем набор битовых строк одинаковой длины. При этом длина машинного слова позволяет нам этими числами оперировать: в word-RAM параметр длины слова $w \geqslant k$.
* $a_i$ - битовая строка длины $k$.

#### *Алгоритм:*
* В массив $u$ длины $U$  в ячейку с индексом $j$ записывается количетсво таких $i \leq n$, что $a_i = j$, то есть сколько раз число $j$ встретилось в массиве $a$. (*Оценка времени:* $O(n)$ - *один проход по массиву* $a$)
* Восстановление отсортированной последовательности: еще раз пройдем по массиву $u$ и каждый индекс $j$ выведем $a[j]$ раз. (*Оценка времени:* $O(U)$ - *один проход по массиву* $u$)

***Оценка времени:*** $O(n + U)$

#### Задача:
Дан массив пар $\{(a_1, b_1), (a_2, b_2), ... , (a_n, b_n)\}$, $a_i, b_i \in [0, U - 1]$. С помощью сортировки подсчетом нужно упорядочить эти пары так, что пары сначала сравниваются по первому элементу, а потом по второму.
*Решение:* Отсортируем сначала по второму элементу в паре, а потом по первому, при этом будем использовать устойчивую сортировку. *(Аналогично задача решается и для кортежей произвольной длины).*

### Поразрядная сортировка (Radix sort)
Рассмотренная выше сортировка подсчетом имеет существенный недостаток: под каждое возможное значение числа $a_i$ нам нужно выделить ячейку в массиве $u$. Поразрядная сортировка лишена этого недостатка.
#### *Алгоритм:*
*Дан массив двоичных строк: 011, 101, 001, 010, 110. Нужно его отсортировать.*
1. Создаем пустые массивы, количество которых равно числу возможных значений одного разряда $a_i$.
*В нашем случае  $a_i$ - двоичное слово, поэтому возможны всего два значения одного разряда - 0 или 1*
2. Распределяем исходные числа по этим спискам в зависимости от величины младшего разряда (по возрастанию). То есть помещаем число в $i$-ый массив , если его младший разряд равен $i$.
В нашем случае массивы будут выглядеть следующим образом:)

	| i = 0 | i = 1 |
	|--|--|
	| 010 | 011 |
	| 110 | 101 |
	|  -  | 001 |
3. Идем по массивам в порядке возрастания $i$ и записываем числа в той последовательности, в которой они встречаются.
*Получим следующую последовательность: 010, 110, 011, 101, 001*
4. Повторяем пункты 2 и 3 для всех более старших разрядов поочередно.
*В нашем случае придется проделать эти операции еще 2 раза, так как всего разрядов - 3. Отсортированная последовательсность будет выглядеть так: 001, 010, 011, 101, 110*

***Оценка времени:*** $O(n + U')$, где $U'$ - количество возможных значений одного разряда.
#### Оптимизация:
* Для случая с битовыми строками оценка времени работы поразрядной сортировки будет $O(n \log U)$ - заметим, что такая оценка невыгодн для нас, всегда можно отсортировать быстрее. за $O(n \log n)$
* Тогда выберем такое $k'$, что $2^{k'} \approx n$, и разобьем биты строки на блоки длины $k'$. Получается, что теперь мы сортируем кортежи длиной $\frac{k}{k'} = \frac{\log U}{\log n}$ *(подобная задача рассмотрена выше)*.
* Новая оценка времени будет: $\frac{\log U}{\log n} \cdot \theta(n + U') = \frac{\log U}{\log n} \cdot \theta(n + n) = O(n\frac{\log U}{\log n})$, а это уже лучше, чем $O(n \log n)$ при $U < n^2$.
***Итоговая оценка времени:*** $O(n\frac{\log U}{\log n})$

### Корзинная или карманная сортировка (Bucket sort)
#### *Алгоритм:*
* Выберем $k'$ и поделим отрезок $[0, U - 1]$  на $2^{k'}$ "корзин" или отрезков.
* Идем по данной последовательсности $a_1, a_2, ..., a_n$ и для каждого $a_i$ определяем, в какую корзину он попадет, и по этому признаку разбиваем последовательность на $2^{k'}$ групп или "корзин".
* Для каждой корзины сортируем попавшие в нее числа и объединяем результат. Сортировать можно, как корзинной сортировкой, так и любой другой.

#### *Оценка времени:*
В самом худшем случае алгоритм будет работать за $O(n \log U)$, но худший случай встречается очень редко или даже почти никогда. Чтобы оценить среднее время работы $t(n)$, решим следующую задачу:

* Дана последовательность вещественных чисел $x_1, x_2, ... , x_n$, случайно сгенерированных на отрезке $[0, 1]$. Для сортировки будем использовать следующий алгоритм:
* На первом шаге применим корзинную сортировку. Разобьем отрезок $[0, 1]$ на $n$ корзин. Распределим $x_i$ по корзинам.
* На втором шаге внутри каждой корзины отсортируем пузырьком

Найдем математическое ожидание времени работы данного алгоритма. Пусть $s_i$ - количество элементов, которые попали в $i$-ую корзину, a $a, b$ - некоторые константы:
	 $$Et(n) = an + E \sum_{i = 1}^n b\cdot s_i^2 = an + E \sum_{i = 1}^n b\cdot s_is_i$$
	 Введем обозначение $A_{ij}$, которое отвечает за событие: элемент $i$ в корзине $j$, и $I_{A_{ij}}$ - индикатор события $A_{ij}$, который равен 1, если событие произошло и 0 иначе. тогда:
	 $$Et(n) = an + E \sum_{i = 1}^n b(\sum_{j = 1}^n I_{A_{ij}}) (\sum_{j = 1}^n I_{A_{ij}}) = an + E\sum_{j = 1}^n\sum_{k = 1}^n\sum_{i = 1}^nb(A_{ij} \cap A_{ik})$$
	 Сумма $\sum_{i = 1}^n(A_{ij} \cap A_{ik})$ принимает значения либо 0, либо 1, поэтому можем переписать, используя событие $B_{jk}$ - элементы $j$ и $k$ в одной корзине:
	 $$Et(n) = an + E\sum_{j = 1}^n\sum_{k = 1}^nb\cdot I_{B_{jk}}$$
	 Разложим матожидание на сумму двух слагаемых, когда элементы $j$ и $k$ совпадают и не совпадают:
	 $$Et(n) = an + E\sum_{j = 1}^nb\cdot I_{B_{jj}} + E\sum_{j < k}^n2b\cdot I_{B_{jk}}$$
	Тогда $Et(n)$ можно оценить следующим образом:
	$$Et(n)\leqslant an + bn + bn^2 \cdot \frac{1}{n} = an + bn + bn = n (a + 2b)$$
	Так как $a, b$ - константы то **время работы линейно**.
